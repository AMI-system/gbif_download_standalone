{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import json\n",
    "import os\n",
    "import shutil\n",
    "import urllib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "from dwca.read import DwCAReader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the arguments\n",
    "home_dir = os.getcwd()\n",
    "\n",
    "species_checklist_path = os.path.join(home_dir,\"species_checklists\",\"uksi-moths-keys-nodup-small.csv\")\n",
    "\n",
    "dwca_occurrence_df_path = \"/Users/lbokeria/Documents/projects/gbif-species-trainer-data/occurrence_dataframes/\"\n",
    "\n",
    "dwca_multimedia_df_path = \"/Users/lbokeria/Documents/projects/gbif-species-trainer-data/dwca_files/multimedia_lepidoptera.csv\"\n",
    "\n",
    "output_location_path = \"/Users/lbokeria/Documents/projects/gbif-species-trainer-data/gbif_images/sandbox/\"\n",
    "\n",
    "max_data_sp = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the multimedia file\n",
    "media_df = pd.read_csv(dwca_multimedia_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read species list\n",
    "moth_data = pd.read_csv(species_checklist_path)\n",
    "\n",
    "taxon_keys = list(moth_data[\"accepted_taxon_key\"])\n",
    "taxon_keys = [int(taxon) for taxon in taxon_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_meta_data(data: pd.DataFrame):\n",
    "    \"\"\"returns the relevant metadata for a GBIF observation\"\"\"\n",
    "\n",
    "    fields = [\n",
    "        \"decimalLatitude\",\n",
    "        \"decimalLongitude\",\n",
    "        \"order\",\n",
    "        \"family\",\n",
    "        \"genus\",\n",
    "        \"species\",\n",
    "        \"acceptedScientificName\",\n",
    "        \"year\",\n",
    "        \"month\",\n",
    "        \"day\",\n",
    "        \"datasetName\",\n",
    "        \"taxonID\",\n",
    "        \"acceptedTaxonKey\",\n",
    "        \"lifeStage\",\n",
    "        \"basisOfRecord\",\n",
    "    ]\n",
    "\n",
    "    meta_data = {}\n",
    "\n",
    "    for field in fields:\n",
    "        if pd.isna(data[field]):\n",
    "            meta_data[field] = \"NA\"\n",
    "        else:\n",
    "            meta_data[field] = data[field]\n",
    "\n",
    "    return meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_image_data(i_taxon_key: int):\n",
    "    species_meta_data = {}\n",
    "\n",
    "    # get taxa information specific to the species\n",
    "    taxon_data = moth_data[moth_data[\"accepted_taxon_key\"] == i_taxon_key]\n",
    "\n",
    "    family_name         = taxon_data[\"family_name\"].item()\n",
    "    genus_name          = taxon_data[\"genus_name\"].item()\n",
    "    species_name        = taxon_data[\"gbif_species_name\"].item()\n",
    "    write_location      = os.path.join(output_location_path,family_name,genus_name,species_name)\n",
    "\n",
    "    # Read the occurrence dataframe\n",
    "    if os.path.isfile(os.path.join(dwca_occurrence_df_path,\n",
    "                                    str(i_taxon_key) + \".csv\")): \n",
    "        i_occ_df = pd.read_csv(os.path.join(dwca_occurrence_df_path,\n",
    "                                            str(i_taxon_key) + \".csv\"))\n",
    "        total_occ = len(i_occ_df)\n",
    "        print(f\"Downloading for {species_name}\", flush=True) \n",
    "    else:\n",
    "        print(f\"No occurrence csv file found for {species_name}, taxon key {i_taxon_key}\")\n",
    "        return\n",
    "\n",
    "    # creating hierarchical folder structure for image storage\n",
    "    if not os.path.isdir(write_location):\n",
    "        try:\n",
    "            os.makedirs(write_location)\n",
    "        except:\n",
    "            print(f\"Could not create the directory for {write_location}\", flush=True)\n",
    "            return\n",
    "        \n",
    "    image_count = 0\n",
    "\n",
    "    if total_occ != 0:\n",
    "        for idx, row in i_occ_df.iterrows():\n",
    "            obs_id = row[\"id\"]\n",
    "\n",
    "            # check occurrence entry in media dataframe\n",
    "            try:\n",
    "                media_entry = media_df.loc[media_df[\"coreid\"] == obs_id]\n",
    "\n",
    "                if not media_entry.empty:\n",
    "                \n",
    "                    if len(media_entry) > 1:  # multiple images for an observation\n",
    "                        media_entry = media_entry.iloc[0, :]\n",
    "                        image_url = media_entry[\"identifier\"]\n",
    "                    else:\n",
    "                        image_url = media_entry[\"identifier\"].item()\n",
    "                else:\n",
    "                    continue\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(e, flush=True)\n",
    "                continue\n",
    "\n",
    "            # download image\n",
    "            if os.path.isfile(write_location + \"/\" + str(obs_id) + \".jpg\"):\n",
    "                image_count += 1\n",
    "            else:\n",
    "                try:\n",
    "                    urllib.request.urlretrieve(\n",
    "                        image_url, write_location + \"/\" + str(obs_id) + \".jpg\"\n",
    "                    )\n",
    "                    image_count += 1\n",
    "                    # m_data = fetch_meta_data(row)\n",
    "                    # meta_data[str(obs_id) + \".jpg\"] = m_data\n",
    "                except:\n",
    "                    print(f\"Error downloading URL: '{image_url}'\")\n",
    "                    continue\n",
    "                \n",
    "            # Get meta data for this occurrence\n",
    "            occ_meta_data = fetch_meta_data(row)\n",
    "            species_meta_data[str(obs_id) + \".jpg\"] = occ_meta_data\n",
    "            \n",
    "            if image_count >= max_data_sp:\n",
    "                break\n",
    "            \n",
    "        # Dump metadata\n",
    "        with open(write_location + \"/\" + \"meta_data.json\", \"w\") as outfile:\n",
    "            json.dump(species_meta_data, outfile)            \n",
    "            \n",
    "    print(f\"Downloading complete for {species_name} with {image_count} images.\",\n",
    "            flush=True)\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for count, i_taxon_key in enumerate(taxon_keys):\n",
    "    \n",
    "    fetch_image_data(i_taxon_key)\n",
    "    \n",
    "print(\"Finished downloading for the given list!\", flush=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, define a function that manages the parallel execution:\n",
    "def download_images_concurrently(taxon_keys):\n",
    "    \n",
    "    with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        begin = time.time()\n",
    "        \n",
    "        # You can use the executor to parallelize your function call:\n",
    "        results = list(executor.map(fetch_image_data, taxon_keys))\n",
    "        \n",
    "        end = time.time()\n",
    "    \n",
    "    print(\"Finished downloading for the given list! Time taken:\", \n",
    "          round(end - begin), \n",
    "          \"seconds\",\n",
    "          flush=True)\n",
    "\n",
    "# Lastly, call the function with your taxon keys:\n",
    "download_images_concurrently(taxon_keys)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gbif_download_standalone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
