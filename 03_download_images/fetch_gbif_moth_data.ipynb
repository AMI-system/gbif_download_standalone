{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the arguments\n",
    "home_dir = os.getcwd()\n",
    "\n",
    "if sys.platform.startswith(\"linux\"):\n",
    "    data_dir = \"/bask/projects/v/vjgo8416-amber/data/gbif-species-trainer-AMI-fork/\"\n",
    "elif sys.platform == \"darwin\":\n",
    "    data_dir = \"/Users/lbokeria/Documents/projects/gbif-species-trainer-data/\"\n",
    "else:\n",
    "    print(\"Not linux or mac!\")\n",
    "\n",
    "species_checklist_path = os.path.join(\"../\",\"species_checklists\",\"uksi-moths-keys-nodup-small.csv\")\n",
    "\n",
    "dwca_occurrence_df_path = os.path.join(data_dir,\"occurrence_dataframes\")\n",
    "\n",
    "dwca_multimedia_df_path = os.path.join(data_dir,\"dwca_files\",\"multimedia_lepidoptera.csv\")\n",
    "\n",
    "output_location_path = os.path.join(data_dir,\"gbif_images\",\"sandbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the multimedia file\n",
    "media_df = pd.read_csv(dwca_multimedia_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read species list\n",
    "moth_data = pd.read_csv(species_checklist_path)\n",
    "\n",
    "taxon_keys = list(moth_data[\"accepted_taxon_key\"])\n",
    "taxon_keys = [int(taxon) for taxon in taxon_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_meta_data(data: pd.DataFrame):\n",
    "    \"\"\"returns the relevant metadata for a GBIF observation\"\"\"\n",
    "\n",
    "    fields = [\n",
    "        \"decimalLatitude\",\n",
    "        \"decimalLongitude\",\n",
    "        \"order\",\n",
    "        \"family\",\n",
    "        \"genus\",\n",
    "        \"species\",\n",
    "        \"acceptedScientificName\",\n",
    "        \"year\",\n",
    "        \"month\",\n",
    "        \"day\",\n",
    "        \"datasetName\",\n",
    "        \"taxonID\",\n",
    "        \"acceptedTaxonKey\",\n",
    "        \"lifeStage\",\n",
    "        \"basisOfRecord\",\n",
    "    ]\n",
    "\n",
    "    meta_data = {}\n",
    "\n",
    "    for field in fields:\n",
    "        if pd.isna(data[field]):\n",
    "            meta_data[field] = \"NA\"\n",
    "        else:\n",
    "            meta_data[field] = data[field]\n",
    "\n",
    "    return meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger():\n",
    "    \n",
    "    # Specify the directory where you want to save the log files\n",
    "    log_dir = \"log_files\"\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    # Use the timestamp string to create a unique filename for the log file\n",
    "    timestamp    = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_filename = os.path.join(log_dir, f'download_log_{timestamp}.log')\n",
    "    \n",
    "    # Get the root logger\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    # If logger has handlers, clear them\n",
    "    for handler in logger.handlers[:]:\n",
    "        handler.close()\n",
    "        logger.removeHandler(handler)\n",
    "    \n",
    "    # Configure the logger\n",
    "    logging.basicConfig(filename=log_filename, level=logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_image_data(i_taxon_key: int):\n",
    "    global skip_non_adults, max_data_sp\n",
    "    \n",
    "    # get taxa information specific to the species\n",
    "    taxon_data = moth_data[moth_data[\"accepted_taxon_key\"] == i_taxon_key]\n",
    "\n",
    "    family_name         = taxon_data[\"family_name\"].item()\n",
    "    genus_name          = taxon_data[\"genus_name\"].item()\n",
    "    species_name        = taxon_data[\"gbif_species_name\"].item()\n",
    "    write_location      = os.path.join(\n",
    "        output_location_path,family_name,genus_name,species_name\n",
    "        )\n",
    "\n",
    "    # Does meta_data exist for this species?\n",
    "    if os.path.isfile(os.path.join(write_location,\"meta_data.json\")):\n",
    "        # Load it \n",
    "        with open(os.path.join(write_location,\"meta_data.json\")) as file:\n",
    "            species_meta_data = json.load(file)\n",
    "    else:\n",
    "        # Creat it\n",
    "        species_meta_data = {}\n",
    "\n",
    "\n",
    "    # Read the occurrence dataframe\n",
    "    if os.path.isfile(os.path.join(dwca_occurrence_df_path,\n",
    "                                    str(i_taxon_key) + \".csv\")): \n",
    "        i_occ_df = pd.read_csv(os.path.join(dwca_occurrence_df_path,\n",
    "                                            str(i_taxon_key) + \".csv\"))\n",
    "        total_occ = len(i_occ_df)\n",
    "        print(f\"Downloading for {species_name}\", flush=True) \n",
    "    else:\n",
    "        # logger.warning(\n",
    "        #     f\"No occurrence csv file found for {species_name}, taxon key {i_taxon_key}\"\n",
    "        #     )\n",
    "        return\n",
    "\n",
    "    # creating hierarchical folder structure for image storage\n",
    "    if not os.path.isdir(write_location):\n",
    "        try:\n",
    "            os.makedirs(write_location)\n",
    "        except:\n",
    "            print(f\"Could not create the directory for {write_location}\", flush=True)\n",
    "            return\n",
    "        \n",
    "    image_count = 0\n",
    "\n",
    "    if total_occ != 0:\n",
    "        # print(f\"{species_name} has some occurrences\")\n",
    "        \n",
    "        for idx, row in i_occ_df.iterrows():\n",
    "            \n",
    "            if skip_non_adults:\n",
    "                \n",
    "                if (not pd.isna(row[\"lifeStage\"])) & (row[\"lifeStage\"] != \"Adult\"):\n",
    "                    \n",
    "                    # print(\"Life stage is\", row[\"lifeStage\"], \"skipping...\")\n",
    "                \n",
    "                    continue\n",
    "            \n",
    "            obs_id = row[\"id\"]\n",
    "\n",
    "            # Is there already an image, or is corrupt or a thumbnail, or broken URL?\n",
    "            if len(species_meta_data) != 0:\n",
    "                \n",
    "                if str(obs_id)+\".jpg\" in species_meta_data.keys():\n",
    "                    \n",
    "                    if species_meta_data[str(obs_id)+\".jpg\"][\"image_is_downloaded\"]:\n",
    "                        # print(f\"{obs_id} already downloaded\")\n",
    "                        image_count += 1\n",
    "                        \n",
    "                        if image_count >= max_data_sp:\n",
    "                            break            \n",
    "                        else:           \n",
    "                            continue    \n",
    "                    \n",
    "                    if (\n",
    "                        (not species_meta_data[str(obs_id)+\".jpg\"][\"image_url_works\"]) or \n",
    "                        species_meta_data[str(obs_id)+\".jpg\"][\"image_is_corrupted\"] or \n",
    "                        species_meta_data[str(obs_id)+\".jpg\"][\"image_is_thumbnail\"]\n",
    "                    ):\n",
    "                        # print(f\"{obs_id} already downloaded/corrupt/thumbnail/broken URL\")\n",
    "                        continue\n",
    "            \n",
    "            # check occurrence entry in media dataframe\n",
    "            try:\n",
    "                media_entry = media_df.loc[media_df[\"coreid\"] == obs_id]\n",
    "\n",
    "                if not media_entry.empty:\n",
    "                    # print(f\"{species_name} has some media\")\n",
    "                          \n",
    "                    if len(media_entry) > 1:  # multiple images for an observation\n",
    "                        media_entry = media_entry.iloc[0, :]\n",
    "                        image_url = media_entry[\"identifier\"]\n",
    "                    else:\n",
    "                        image_url = media_entry[\"identifier\"].item()\n",
    "                else:\n",
    "                    \n",
    "                    # print(f\"{species_name} has NO media\")\n",
    "                    continue\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(e, flush=True)\n",
    "                continue\n",
    "\n",
    "            # download image\n",
    "            try:\n",
    "                urllib.request.urlretrieve(\n",
    "                    image_url, write_location + \"/\" + str(obs_id) + \".jpg\"\n",
    "                )\n",
    "                image_count += 1\n",
    "                url_works = True\n",
    "                image_downloaded = True\n",
    "\n",
    "            except:\n",
    "                print(f\"Error downloading URL: '{image_url}'\")\n",
    "                url_works = False\n",
    "                image_downloaded = False\n",
    "            \n",
    "            # Get meta data for this occurrence\n",
    "            occ_meta_data = fetch_meta_data(row)\n",
    "            occ_meta_data[\"image_is_downloaded\"] = image_downloaded\n",
    "            occ_meta_data[\"image_url_works\"] = url_works\n",
    "            occ_meta_data[\"image_is_corrupted\"] = \"\"\n",
    "            occ_meta_data[\"image_is_thumbnail\"] = \"\"\n",
    "            \n",
    "            species_meta_data[str(obs_id) + \".jpg\"] = occ_meta_data\n",
    "            \n",
    "            if image_count >= max_data_sp:\n",
    "                break\n",
    "            \n",
    "        # Dump metadata\n",
    "        with open(write_location + \"/\" + \"meta_data.json\", \"w\") as outfile:\n",
    "            json.dump(species_meta_data, outfile)            \n",
    "            \n",
    "    print(f\"Downloading complete for {species_name} with {image_count} images.\",\n",
    "            flush=True)\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def download_images_concurrently(taxon_keys,use_parallel,use_multiproc,n_workers):\n",
    "    \n",
    "    begin = time.time()\n",
    "\n",
    "    \n",
    "    if use_parallel:\n",
    "        \n",
    "        if use_multiproc:\n",
    "\n",
    "            with ProcessPoolExecutor(max_workers=n_workers) as executor:            \n",
    "\n",
    "                # You can use the executor to parallelize your function call:\n",
    "                executor.map(fetch_image_data, taxon_keys)\n",
    "\n",
    "        else:\n",
    "            \n",
    "            with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "\n",
    "                # You can use the executor to parallelize your function call:\n",
    "                results = list(executor.map(fetch_image_data, taxon_keys))\n",
    "    \n",
    "    else:\n",
    "       \n",
    "        for i_taxon_key in taxon_keys:\n",
    "            print(f\"Calling for {i_taxon_key}\")\n",
    "            fetch_image_data(i_taxon_key)\n",
    "   \n",
    "\n",
    "    end = time.time()\n",
    "            \n",
    "    print(\"Finished downloading for the given list! Time taken:\", \n",
    "          round(end - begin), \n",
    "          \"seconds\",\n",
    "          flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading for Bembecia ichneumoniformis\n",
      "Downloading for Sesia bembeciformis\n",
      "Downloading for Pennisetia hylaeiformis\n",
      "Downloading for Paranthrene tabaniformis\n",
      "Downloading for Pyropteron chrysidiformis\n",
      "Downloading for Synanthedon andrenaeformisDownloading for Synanthedon culiciformis\n",
      "Downloading for Synanthedon formicaeformis\n",
      "\n",
      "Downloading for Sesia apiformis\n",
      "Downloading for Synanthedon flaviventris\n",
      "Error downloading URL: 'https://www.ksib.pl/ipt-nadlsiewierz/034-11-Synanthedon_formicaeformis_(3424).jpg'\n",
      "Error downloading URL: 'https://www.ksib.pl/ipt-nadlsiewierz/034-06-Synanthedon_andrenaeformis_(3405).jpg'\n",
      "Downloading complete for Pyropteron chrysidiformis with 0 images.\n",
      "Downloading for Synanthedon myopaeformis\n",
      "Downloading complete for Synanthedon flaviventris with 0 images.\n",
      "Downloading for Synanthedon scoliaeformis\n",
      "Error downloading URL: 'https://www.ksib.pl/ipt-nadlsiewierz/034-15-Synanthedon_scoliaeformis_(3434b).jpg'\n",
      "Downloading complete for Synanthedon andrenaeformis with 10 images.\n",
      "Downloading for Synanthedon spheciformis\n",
      "Error downloading URL: 'https://www.ksib.pl/ipt-nadlsiewierz/034-16-Synanthedon_spheciformis_(3435).jpg'\n",
      "Error downloading URL: 'https://www.ksib.pl/ipt-nadlsiewierz/034-05-Paranthrene_tabaniformis_(3401).jpg'\n",
      "Error downloading URL: 'nan'\n",
      "Error downloading URL: 'nan'\n",
      "Error downloading URL: 'nan'\n",
      "Error downloading URL: 'nan'\n",
      "Error downloading URL: 'nan'\n",
      "Error downloading URL: 'https://www.ksib.pl/ipt-nadlsiewierz/034-01-Pennisetia_hylaeiformis_(3381).jpg'\n",
      "Error downloading URL: 'https://www.ksib.pl/ipt-nadlsiewierz/034-09-Synanthedon_culiciformis_(3415).jpg'\n",
      "Downloading complete for Synanthedon scoliaeformis with 10 images.\n",
      "Downloading for Synanthedon tipuliformis\n",
      "Error downloading URL: 'https://www.ksib.pl/ipt-nadlsiewierz/034-14-Synanthedon_myopaeformis_(3431).jpg'\n",
      "Error downloading URL: 'https://static.inaturalist.org/photos/79531523/original.jpg'\n",
      "Downloading complete for Synanthedon culiciformis with 10 images.\n",
      "Downloading for Synanthedon vespiformis\n",
      "Error downloading URL: 'https://www.ksib.pl/ipt-nadlsiewierz/034-19-Synanthedon_vespiformis_(3442).jpg'\n",
      "Downloading complete for Synanthedon formicaeformis with 10 images.\n",
      "Downloading complete for Synanthedon spheciformis with 10 images.\n",
      "Downloading complete for Sesia bembeciformis with 10 images.\n",
      "Error downloading URL: 'https://www.ksib.pl/ipt-nadlsiewierz/034-02-Sesia_apiformis_(3389).jpg'\n",
      "Downloading complete for Paranthrene tabaniformis with 10 images.\n",
      "Downloading complete for Synanthedon myopaeformis with 10 images.\n",
      "Downloading complete for Synanthedon vespiformis with 10 images.\n",
      "Error downloading URL: 'https://www.ksib.pl/ipt-nadlsiewierz/034-20-Bembecia_ichneumoniformis_(3447).jpg'\n",
      "Error downloading URL: 'http://cettia-idf.fr/ajax/dldimg/img_user!2017!07!761!1500276269_761.jpg'\n",
      "Error downloading URL: 'https://static.inaturalist.org/photos/151585772/original.jpg'\n",
      "Error downloading URL: 'https://static.inaturalist.org/photos/86344954/original.jpg'\n",
      "Downloading complete for Pennisetia hylaeiformis with 10 images.\n",
      "Downloading complete for Sesia apiformis with 10 images.\n",
      "Error downloading URL: 'https://www.ksib.pl/ipt-nadlsiewierz/034-18-Synanthedon_tipuliformis_(3441b).jpg'\n",
      "Downloading complete for Synanthedon tipuliformis with 10 images.\n",
      "Downloading complete for Bembecia ichneumoniformis with 10 images.\n",
      "Finished downloading for the given list! Time taken: 68 seconds\n"
     ]
    }
   ],
   "source": [
    "# Setup logger\n",
    "setup_logger()\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Start the run\n",
    "n_workers       = 10\n",
    "use_parallel    = True\n",
    "use_multiproc   = False\n",
    "max_data_sp     = 10\n",
    "skip_non_adults = True\n",
    "\n",
    "# Lastly, call the function with your taxon keys:\n",
    "download_images_concurrently(taxon_keys,use_parallel,use_multiproc,n_workers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gbif_download_standalone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
