{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import urllib\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "import time\n",
    "import sys\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import logging\n",
    "import datetime\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the arguments\n",
    "home_dir = os.getcwd()\n",
    "\n",
    "if sys.platform.startswith(\"linux\"):\n",
    "    data_dir = \"/bask/projects/v/vjgo8416-amber/data/gbif-species-trainer-AMI-fork/\"\n",
    "elif sys.platform == \"darwin\":\n",
    "    data_dir = \"/Users/lbokeria/Documents/projects/gbif-species-trainer-data/\"\n",
    "else:\n",
    "    print(\"Not linux or mac!\")\n",
    "\n",
    "species_checklist_path = os.path.join(home_dir,\"species_checklists\",\"uksi-moths-keys-nodup-small.csv\")\n",
    "\n",
    "dwca_occurrence_df_path = os.path.join(data_dir,\"occurrence_dataframes\")\n",
    "\n",
    "dwca_multimedia_df_path = os.path.join(data_dir,\"dwca_files\",\"multimedia_lepidoptera.csv\")\n",
    "\n",
    "output_location_path = os.path.join(data_dir,\"gbif_images\",\"sandbox\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the multimedia file\n",
    "media_df = pd.read_csv(dwca_multimedia_df_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read species list\n",
    "moth_data = pd.read_csv(species_checklist_path)\n",
    "\n",
    "taxon_keys = list(moth_data[\"accepted_taxon_key\"])\n",
    "taxon_keys = [int(taxon) for taxon in taxon_keys]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_meta_data(data: pd.DataFrame):\n",
    "    \"\"\"returns the relevant metadata for a GBIF observation\"\"\"\n",
    "\n",
    "    fields = [\n",
    "        \"decimalLatitude\",\n",
    "        \"decimalLongitude\",\n",
    "        \"order\",\n",
    "        \"family\",\n",
    "        \"genus\",\n",
    "        \"species\",\n",
    "        \"acceptedScientificName\",\n",
    "        \"year\",\n",
    "        \"month\",\n",
    "        \"day\",\n",
    "        \"datasetName\",\n",
    "        \"taxonID\",\n",
    "        \"acceptedTaxonKey\",\n",
    "        \"lifeStage\",\n",
    "        \"basisOfRecord\",\n",
    "    ]\n",
    "\n",
    "    meta_data = {}\n",
    "\n",
    "    for field in fields:\n",
    "        if pd.isna(data[field]):\n",
    "            meta_data[field] = \"NA\"\n",
    "        else:\n",
    "            meta_data[field] = data[field]\n",
    "\n",
    "    return meta_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Then, define a function that manages the parallel execution:\n",
    "def download_images_concurrently(taxon_keys,use_parallel,n_workers):\n",
    "    \n",
    "    begin = time.time()\n",
    "\n",
    "    \n",
    "    if use_parallel:\n",
    "        with ThreadPoolExecutor(max_workers=n_workers) as executor:\n",
    "\n",
    "            # You can use the executor to parallelize your function call:\n",
    "            results = list(executor.map(fetch_image_data, taxon_keys))\n",
    "    \n",
    "    else:\n",
    "       \n",
    "        for i_taxon_key in taxon_keys:\n",
    "            print(f\"Calling for {i_taxon_key}\")\n",
    "            fetch_image_data(i_taxon_key)\n",
    "   \n",
    "\n",
    "    end = time.time()\n",
    "            \n",
    "    print(\"Finished downloading for the given list! Time taken:\", \n",
    "          round(end - begin), \n",
    "          \"seconds\",\n",
    "          flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_logger():\n",
    "    \n",
    "    # Specify the directory where you want to save the log files\n",
    "    log_dir = \"log_files\"\n",
    "\n",
    "    # Ensure the directory exists\n",
    "    if not os.path.exists(log_dir):\n",
    "        os.makedirs(log_dir)\n",
    "\n",
    "    # Use the timestamp string to create a unique filename for the log file\n",
    "    timestamp    = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    log_filename = os.path.join(log_dir, f'download_log_{timestamp}.log')\n",
    "    \n",
    "    # Get the root logger\n",
    "    logger = logging.getLogger()\n",
    "    \n",
    "    # If logger has handlers, clear them\n",
    "    for handler in logger.handlers[:]:\n",
    "        handler.close()\n",
    "        logger.removeHandler(handler)\n",
    "    \n",
    "    # Configure the logger\n",
    "    logging.basicConfig(filename=log_filename, level=logging.INFO,\n",
    "                        format='%(asctime)s - %(levelname)s - %(message)s')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_image_data(i_taxon_key: int):\n",
    "    global skip_non_adults, max_data_sp\n",
    "    \n",
    "    # get taxa information specific to the species\n",
    "    taxon_data = moth_data[moth_data[\"accepted_taxon_key\"] == i_taxon_key]\n",
    "\n",
    "    family_name         = taxon_data[\"family_name\"].item()\n",
    "    genus_name          = taxon_data[\"genus_name\"].item()\n",
    "    species_name        = taxon_data[\"gbif_species_name\"].item()\n",
    "    write_location      = os.path.join(output_location_path,family_name,genus_name,species_name)\n",
    "\n",
    "    # Does meta_data exist for this species?\n",
    "    if os.path.isfile(os.path.join(write_location,\"meta_data.json\")):\n",
    "        # Load it \n",
    "        with open(os.path.join(write_location,\"meta_data.json\")) as file:\n",
    "            species_meta_data = json.load(file)\n",
    "    else:\n",
    "        # Creat it\n",
    "        species_meta_data = {}\n",
    "\n",
    "\n",
    "    # Read the occurrence dataframe\n",
    "    if os.path.isfile(os.path.join(dwca_occurrence_df_path,\n",
    "                                    str(i_taxon_key) + \".csv\")): \n",
    "        i_occ_df = pd.read_csv(os.path.join(dwca_occurrence_df_path,\n",
    "                                            str(i_taxon_key) + \".csv\"))\n",
    "        total_occ = len(i_occ_df)\n",
    "        print(f\"Downloading for {species_name}\", flush=True) \n",
    "    else:\n",
    "        logger.warning(\n",
    "            f\"No occurrence csv file found for {species_name}, taxon key {i_taxon_key}\"\n",
    "            )\n",
    "        return\n",
    "\n",
    "    # creating hierarchical folder structure for image storage\n",
    "    if not os.path.isdir(write_location):\n",
    "        try:\n",
    "            os.makedirs(write_location)\n",
    "        except:\n",
    "            print(f\"Could not create the directory for {write_location}\", flush=True)\n",
    "            return\n",
    "        \n",
    "    image_count = 0\n",
    "\n",
    "    if total_occ != 0:\n",
    "        # print(f\"{species_name} has some occurrences\")\n",
    "        \n",
    "        for idx, row in i_occ_df.iterrows():\n",
    "            \n",
    "            if skip_non_adults:\n",
    "                \n",
    "                if (not pd.isna(row[\"lifeStage\"])) & (row[\"lifeStage\"] != \"Adult\"):\n",
    "                    \n",
    "                    # print(\"Life stage is\", row[\"lifeStage\"], \"skipping...\")\n",
    "                \n",
    "                    continue\n",
    "            \n",
    "            obs_id = row[\"id\"]\n",
    "\n",
    "            # Is there already an image, or is corrupt or a thumbnail, or broken URL?\n",
    "            if len(species_meta_data) != 0:\n",
    "                \n",
    "                if str(obs_id)+\".jpg\" in species_meta_data.keys():\n",
    "                    \n",
    "                    if species_meta_data[str(obs_id)+\".jpg\"][\"image_is_downloaded\"]:\n",
    "                        # print(f\"{obs_id} already downloaded\")\n",
    "                        image_count += 1\n",
    "                        \n",
    "                        if image_count >= max_data_sp:\n",
    "                            break            \n",
    "                        else:           \n",
    "                            continue    \n",
    "                    \n",
    "                    if (\n",
    "                        (not species_meta_data[str(obs_id)+\".jpg\"][\"image_url_works\"]) or \n",
    "                        species_meta_data[str(obs_id)+\".jpg\"][\"image_is_corrupted\"] or \n",
    "                        species_meta_data[str(obs_id)+\".jpg\"][\"image_is_thumbnail\"]\n",
    "                    ):\n",
    "                        # print(f\"{obs_id} already downloaded/corrupt/thumbnail/broken URL\")\n",
    "                        continue\n",
    "            \n",
    "            # check occurrence entry in media dataframe\n",
    "            try:\n",
    "                media_entry = media_df.loc[media_df[\"coreid\"] == obs_id]\n",
    "\n",
    "                if not media_entry.empty:\n",
    "                    # print(f\"{species_name} has some media\")\n",
    "                          \n",
    "                    if len(media_entry) > 1:  # multiple images for an observation\n",
    "                        media_entry = media_entry.iloc[0, :]\n",
    "                        image_url = media_entry[\"identifier\"]\n",
    "                    else:\n",
    "                        image_url = media_entry[\"identifier\"].item()\n",
    "                else:\n",
    "                    \n",
    "                    # print(f\"{species_name} has NO media\")\n",
    "                    continue\n",
    "                    \n",
    "            except Exception as e:\n",
    "                print(e, flush=True)\n",
    "                continue\n",
    "\n",
    "            # download image\n",
    "            try:\n",
    "                urllib.request.urlretrieve(\n",
    "                    image_url, write_location + \"/\" + str(obs_id) + \".jpg\"\n",
    "                )\n",
    "                image_count += 1\n",
    "                url_works = True\n",
    "                image_downloaded = True\n",
    "                # m_data = fetch_meta_data(row)\n",
    "                # meta_data[str(obs_id) + \".jpg\"] = m_data\n",
    "            except:\n",
    "                print(f\"Error downloading URL: '{image_url}'\")\n",
    "                url_works = False\n",
    "                image_downloaded = False\n",
    "            \n",
    "            # Get meta data for this occurrence\n",
    "            occ_meta_data = fetch_meta_data(row)\n",
    "            occ_meta_data[\"image_is_downloaded\"] = image_downloaded\n",
    "            occ_meta_data[\"image_url_works\"] = url_works\n",
    "            occ_meta_data[\"image_is_corrupted\"] = \"\"\n",
    "            occ_meta_data[\"image_is_thumbnail\"] = \"\"\n",
    "            \n",
    "            species_meta_data[str(obs_id) + \".jpg\"] = occ_meta_data\n",
    "            \n",
    "            if image_count >= max_data_sp:\n",
    "                break\n",
    "            \n",
    "        # Dump metadata\n",
    "        with open(write_location + \"/\" + \"meta_data.json\", \"w\") as outfile:\n",
    "            json.dump(species_meta_data, outfile)            \n",
    "            \n",
    "    print(f\"Downloading complete for {species_name} with {image_count} images.\",\n",
    "            flush=True)\n",
    "    \n",
    "    return\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_taxon_key = 1940807\n",
    "\n",
    "# i_occ_df = pd.read_csv(os.path.join(dwca_occurrence_df_path,\n",
    "#                                     str(i_taxon_key) + \".csv\"))\n",
    "\n",
    "# i_occ_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dwca_occurrence_df_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logger\n",
    "setup_logger()\n",
    "logger = logging.getLogger()\n",
    "\n",
    "# Start the run\n",
    "n_workers = 10\n",
    "use_parallel = True\n",
    "max_data_sp = 300\n",
    "skip_non_adults = True\n",
    "\n",
    "# Lastly, call the function with your taxon keys:\n",
    "download_images_concurrently(taxon_keys,use_parallel,n_workers)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gbif_download_standalone",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
